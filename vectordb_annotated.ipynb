{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0115e112-a5f9-46b7-bf76-1d39552485c3",
   "metadata": {},
   "source": [
    "\n",
    "## This version of build_vector_db.py and asupport_chat.py just have each section of the code clearly deliniated for a class demo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ec241b7-948c-4441-b4fc-000da654e82a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== BUILDING VECTOR DATABASE ===\n",
      "\n",
      "\n",
      "============================================================\n",
      "STEP 0: INGEST RAW DOCUMENTS\n",
      "============================================================\n",
      "Loading documents...\n",
      "  → SupportCo Online Support Personnel Instruction Manual.pdf\n",
      "Loaded 31 sections.\n",
      "\n",
      "============================================================\n",
      "STEP 1: CHUNKING\n",
      "============================================================\n",
      "Created 103 chunks.\n",
      "\n",
      "============================================================\n",
      "STEP 2: EMBEDDING\n",
      "============================================================\n",
      "Generating embeddings...\n",
      "  → Embedded 100/103 chunks\n",
      "Embedded 103 vectors.\n",
      "\n",
      "============================================================\n",
      "STEP 3: INDEXING & STORAGE IN QDRANT\n",
      "============================================================\n",
      "Deleted existing collection: support_docs\n",
      "Collection 'support_docs' created.\n",
      "  → Upserted batch 1\n",
      "  → Upserted batch 2\n",
      "Vector DB built successfully!\n",
      "\n",
      "============================================================\n",
      "SUCCESS! Vector DB is ready!\n",
      "Next: Run 'python support_chat.py' for the AI chat interface.\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================\n",
    "# build_vector_db.py - FINAL WORKING VERSION\n",
    "# PIPELINE: 0 → Ingest | 1 → Chunk | 2 → Embed | 3 → Index\n",
    "# ==============================================================\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "from langchain_community.document_loaders import PyPDFLoader, TextLoader\n",
    "from langchain_community.document_loaders import UnstructuredMarkdownLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http.models import (\n",
    "    Distance,\n",
    "    VectorParams,\n",
    "    HnswConfigDiff,\n",
    "    PointStruct,\n",
    ")\n",
    "\n",
    "# CONFIG\n",
    "DATA_DIR = \"support_docs\"\n",
    "COLLECTION_NAME = \"support_docs\"\n",
    "EMBEDDING_MODEL = \"all-MiniLM-L6-v2\"\n",
    "CHUNK_SIZE = 500\n",
    "CHUNK_OVERLAP = 50\n",
    "QDRANT_URL = \"http://localhost:6333\"\n",
    "\n",
    "\n",
    "# ==============================================================\n",
    "# STEP 0: INGEST RAW DOCUMENTS\n",
    "# Load PDFs, .txt, .md from support_docs/\n",
    "# ==============================================================\n",
    "def load_documents() -> List[Document]:\n",
    "    # ── 1. Print header for clarity in console\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"STEP 0: INGEST RAW DOCUMENTS\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # ── 2. Define where to look for files\n",
    "    data_dir = Path(DATA_DIR)  # DATA_DIR = \"support_docs\"\n",
    "\n",
    "    # ── 3. Safety check: folder must exist\n",
    "    if not data_dir.exists():\n",
    "        raise FileNotFoundError(f\"Create '{DATA_DIR}' folder with your files.\")\n",
    "\n",
    "    # ── 4. Map file extensions → loader classes\n",
    "    loaders = {\n",
    "        \".pdf\": PyPDFLoader,\n",
    "        \".txt\": TextLoader,\n",
    "        \".md\":  UnstructuredMarkdownLoader\n",
    "    }\n",
    "\n",
    "    # ── 5. List to store all loaded document objects\n",
    "    docs = []\n",
    "\n",
    "    # ── 6. Start loading\n",
    "    print(\"Loading documents...\")\n",
    "\n",
    "    # ── 7. Recursively search ALL files in support_docs/ and subfolders\n",
    "    for file_path in data_dir.rglob(\"*\"):\n",
    "        ext = file_path.suffix.lower()  # e.g., \".pdf\"\n",
    "\n",
    "        # ── 8. Only process supported file types\n",
    "        if ext in loaders:\n",
    "            print(f\"  → {file_path.name}\")  # Show filename\n",
    "\n",
    "            # ── 9. Load the file using the correct loader\n",
    "            loader = loaders[ext](str(file_path))  # Create loader instance\n",
    "            for doc in loader.load():              # .load() returns list of Document\n",
    "                # ── 10. Add useful metadata\n",
    "                doc.metadata.update({\n",
    "                    \"source\": file_path.name,         # e.g., \"Supportco Manual.pdf\"\n",
    "                    \"category\": file_path.parent.name # e.g., \"onboarding\", \"troubleshooting\"\n",
    "                })\n",
    "                docs.append(doc)  # Save each page/section as a Document\n",
    "\n",
    "    # ── 11. Summary\n",
    "    print(f\"Loaded {len(docs)} sections.\")\n",
    "    return docs  # Return list of Document objects\n",
    "\n",
    "\n",
    "# ==============================================================\n",
    "# STEP 1: CHUNKING (content-aware)\n",
    "# Split long docs into 500-char chunks with 50 overlap\n",
    "# ==============================================================\n",
    "def chunk_documents(docs: List[Document]) -> List[Document]:\n",
    "    # ── 1. Print header\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"STEP 1: CHUNKING\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # ── 2. Create a text splitter with settings\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=CHUNK_SIZE,      # e.g., 500 characters\n",
    "        chunk_overlap=CHUNK_OVERLAP # e.g., 50 characters\n",
    "    )\n",
    "\n",
    "    # ── 3. Split all documents into smaller chunks\n",
    "    chunks = splitter.split_documents(docs)\n",
    "\n",
    "    # ── 4. Summary\n",
    "    print(f\"Created {len(chunks)} chunks.\")\n",
    "    return chunks  # List of smaller Document objects\n",
    "\n",
    "\n",
    "# ==============================================================\n",
    "# STEP 2: EMBEDDING\n",
    "# Convert each chunk into a 384-dim vector using all-MiniLM-L6-v2\n",
    "# ==============================================================\n",
    "# ==============================================================\n",
    "def embed_chunks(chunks: List[Document]) -> List[dict]:\n",
    "    # ── 1. Print header\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"STEP 2: EMBEDDING\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # ── 2. Load the embedding model (once)\n",
    "    model = SentenceTransformer(EMBEDDING_MODEL)  # e.g., \"all-MiniLM-L6-v2\"\n",
    "\n",
    "    # ── 3. List to store results\n",
    "    embedded = []\n",
    "\n",
    "    # ── 4. Start embedding\n",
    "    print(\"Generating embeddings...\")\n",
    "\n",
    "    # ── 5. Loop through each chunk\n",
    "    for i, c in enumerate(chunks):\n",
    "        # ── 6. Convert text → 384-dimensional vector\n",
    "        vec = model.encode(c.page_content).tolist()\n",
    "\n",
    "        # ── 7. Save: ID, vector, and original text + source\n",
    "        embedded.append({\n",
    "            \"id\": i,                                      # Unique ID\n",
    "            \"vector\": vec,                                # [0.1, -0.3, ..., 0.7] (384 numbers)\n",
    "            \"payload\": {\n",
    "                \"text\": c.page_content,                   # Original chunk\n",
    "                \"source\": c.metadata.get(\"source\")        # e.g., \"Supportco Manual.pdf\"\n",
    "            }\n",
    "        })\n",
    "\n",
    "        # ── 8. Progress update every 100 chunks\n",
    "        if i % 100 == 0 and i > 0:\n",
    "            print(f\"  → Embedded {i}/{len(chunks)} chunks\")\n",
    "\n",
    "    # ── 9. Final summary\n",
    "    print(f\"Embedded {len(embedded)} vectors.\")\n",
    "    return embedded  # List of dicts ready for Qdrant\n",
    "\n",
    "\n",
    "# ==============================================================\n",
    "# STEP 3: INDEXING & STORAGE IN QDRANT\n",
    "# Create collection + upsert all vectors\n",
    "# ==============================================================\n",
    "def build_qdrant_index(points: List[dict]):\n",
    "    # ── 1. Print header\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"STEP 3: INDEXING & STORAGE IN QDRANT\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # ── 2. Connect to Qdrant (running in Docker)\n",
    "    client = QdrantClient(url=QDRANT_URL)  # QDRANT_URL = \"http://localhost:6333\"\n",
    "\n",
    "    # ── 3. Delete old collection if it exists\n",
    "    if client.collection_exists(COLLECTION_NAME):\n",
    "        client.delete_collection(COLLECTION_NAME)\n",
    "        print(f\"Deleted existing collection: {COLLECTION_NAME}\")\n",
    "\n",
    "    # ── 4. Create a new collection\n",
    "    client.create_collection(\n",
    "        collection_name=COLLECTION_NAME,  # e.g., \"support_docs\"\n",
    "        vectors_config=VectorParams(\n",
    "            size=len(points[0][\"vector\"]),   # 384\n",
    "            distance=Distance.COSINE         # Cosine similarity\n",
    "        ),\n",
    "        hnsw_config=HnswConfigDiff(\n",
    "            m=16,                # HNSW: connections per node HNSW = Hierarchical Navigable Small World\n",
    "            ef_construct=200,    # HNSW: search depth during build\n",
    "            full_scan_threshold=10000  # Switch to brute-force if small\n",
    "        ),\n",
    "    )\n",
    "    print(f\"Collection '{COLLECTION_NAME}' created.\")\n",
    "\n",
    "    # ── 5. Upload in batches (faster, safer)\n",
    "    BATCH = 100\n",
    "    for i in range(0, len(points), BATCH):\n",
    "        batch = points[i:i+BATCH]  # Slice: 100 items at a time\n",
    "\n",
    "        # ── 6. Convert dict → Qdrant PointStruct\n",
    "        client.upsert(\n",
    "            collection_name=COLLECTION_NAME,\n",
    "            points=[\n",
    "                PointStruct(\n",
    "                    id=p[\"id\"],\n",
    "                    vector=p[\"vector\"],\n",
    "                    payload=p[\"payload\"]\n",
    "                ) for p in batch\n",
    "            ]\n",
    "        )\n",
    "        print(f\"  → Upserted batch {i//BATCH + 1}\")\n",
    "\n",
    "    # ── 7. Done!\n",
    "    print(\"Vector DB built successfully!\")\n",
    "\n",
    "\n",
    "# ==============================================================\n",
    "# MAIN: Run all steps\n",
    "# ==============================================================\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"=== BUILDING VECTOR DATABASE ===\\n\")\n",
    "    docs = load_documents()\n",
    "    chunks = chunk_documents(docs)\n",
    "    points = embed_chunks(chunks)\n",
    "    build_qdrant_index(points)\n",
    "    Path(\"vector_db_built.flag\").touch()\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"SUCCESS! Vector DB is ready!\")\n",
    "    print(\"Next: Run 'python support_chat.py' for the AI chat interface.\")\n",
    "    print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddbdf326-c743-464a-9ed8-fec2a6c5ea57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "INITIALIZING AI SUPPORT CHAT WITH OPENAI\n",
      "============================================================\n",
      "Loading embedding model...\n",
      "Searching Qdrant vector database...\n",
      "\n",
      " Embedding user query...\n",
      "  → Query vector generated (dim=384)\n",
      "  → Found 3 relevant chunks:\n",
      "     [1] Score: 0.399 | Source: SupportCo Online Support Personnel Instruction Manual.pdf\n",
      "     [2] Score: 0.381 | Source: SupportCo Online Support Personnel Instruction Manual.pdf\n",
      "     [3] Score: 0.337 | Source: SupportCo Online Support Personnel Instruction Manual.pdf\n",
      "\n",
      "Generating answer with OpenAI GPT...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/zr/8xlf44gd1yng__g6g_09_xnr0000gn/T/ipykernel_72512/1716993564.py:90: DeprecationWarning: `search` method is deprecated and will be removed in the future. Use `query_points` instead.\n",
      "  search_result = client.search(\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================\n",
    "# support_chat.py - AI SUPPORT CHAT GUI with OpenAI GPT\n",
    "# Uses .env for API key | Modern OpenAI v1.0+ API\n",
    "# Answers questions using your PDF + GPT-3.5/GPT-4\n",
    "# Requires: vector DB built + Qdrant running\n",
    "# ==============================================================\n",
    "\n",
    "import tkinter as tk\n",
    "from tkinter import scrolledtext, messagebox\n",
    "from typing import List\n",
    "import os\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv  # ← Loads .env file\n",
    "\n",
    "# ================================\n",
    "# LOAD ENVIRONMENT VARIABLES\n",
    "# ================================\n",
    "load_dotenv()  # Reads .env file in project root\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not OPENAI_API_KEY:\n",
    "    raise ValueError(\n",
    "        \"OPENAI_API_KEY not found!\\n\"\n",
    "        \"Create a .env file in your project folder with:\\n\"\n",
    "        \"OPENAI_API_KEY=sk-...\\n\"\n",
    "        \"Or run: export OPENAI_API_KEY='sk-...'\"\n",
    "    )\n",
    "\n",
    "# ================================\n",
    "# CONFIG\n",
    "# ================================\n",
    "EMBEDDING_MODEL = \"all-MiniLM-L6-v2\"\n",
    "QDRANT_URL = \"http://localhost:6333\"\n",
    "COLLECTION_NAME = \"support_docs\"\n",
    "TOP_K = 3\n",
    "OPENAI_MODEL = \"gpt-5-chat-latest\"  # or gpt-3.5-turbo or \"gpt-4o\" for better quality\n",
    "\n",
    "# ================================\n",
    "# INITIALIZE CLIENTS\n",
    "# ================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"INITIALIZING AI SUPPORT CHAT WITH OPENAI\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from qdrant_client import QdrantClient\n",
    "from openai import OpenAI  # ← Modern OpenAI v1.0+\n",
    "\n",
    "print(\"Loading embedding model...\")\n",
    "model = SentenceTransformer(EMBEDDING_MODEL)\n",
    "client = QdrantClient(url=QDRANT_URL)\n",
    "openai_client = OpenAI(api_key=OPENAI_API_KEY)  # ← Secure client\n",
    "\n",
    "\n",
    "# ================================\n",
    "# STEP 5: EMBED USER QUERY\n",
    "# ================================\n",
    "def embed_query(query: str):\n",
    "    # ← Takes the user's question as a string (e.g., \"How do I reset my password?\")\n",
    "\n",
    "    print(\"\\n Embedding user query...\")\n",
    "    # ← Tells you in the console: \"Now turning your question into AI understanding\"\n",
    "\n",
    "    query_vec = model.encode(query).tolist()\n",
    "    # ← THE MAGIC HAPPENS HERE:\n",
    "    #    • model = your local SentenceTransformer (all-MiniLM-L6-v2)\n",
    "    #    • .encode() = converts text → 384-dimensional vector (meaning in numbers)\n",
    "    #    • .tolist() = turns NumPy array → regular Python list (Qdrant needs this)\n",
    "\n",
    "    print(f\"  → Query vector generated (dim={len(query_vec)})\")\n",
    "    # ← Confirms success: \"We now have a 384-number 'fingerprint' of your question\"\n",
    "\n",
    "    return query_vec\n",
    "    # ← Returns the vector so Qdrant can search for similar chunks\n",
    "\n",
    "\n",
    "# ================================\n",
    "# STEP 6 and 7: SEARCH QDRANT; Retrieve and Rank\n",
    "# ================================\n",
    "def search_qdrant(query: str) -> List[str]:\n",
    "    # Takes the user's question (string) and returns a list of text chunks\n",
    "\n",
    "    print(\"Searching Qdrant vector database...\")\n",
    "    # Lets you know in the console: \"Now looking for answers in your manual\"\n",
    "\n",
    "    query_vec = embed_query(query)\n",
    "    # Turns your question into a 384D vector (meaning fingerprint)\n",
    "    # So Qdrant can compare it to the chunks from your PDF\n",
    "\n",
    "    search_result = client.search(\n",
    "        collection_name=COLLECTION_NAME,   # \"support_docs\" — your knowledge base\n",
    "        query_vector=query_vec,            # The vector of your question\n",
    "        limit=TOP_K,                       # Only return top 3 matches (fast + accurate)\n",
    "        with_payload=True,                 # Include the actual text and source\n",
    "    )\n",
    "    # This is the SEARCH — finds chunks most similar in meaning\n",
    "\n",
    "    contexts = []  # Will hold the text chunks to show the user\n",
    "    print(f\"  → Found {len(search_result)} relevant chunks:\")\n",
    "\n",
    "    for i, hit in enumerate(search_result):\n",
    "        # Loop through the top matches\n",
    "        text = hit.payload.get(\"text\", \"\")           # The actual text from the PDF\n",
    "        source = hit.payload.get(\"source\", \"Unknown\") # Which file it came from\n",
    "        score = hit.score                            # How similar (0.0 to 1.0)\n",
    "\n",
    "        print(f\"     [{i+1}] Score: {score:.3f} | Source: {source}\")\n",
    "        # Shows you how good each match is — higher score = better match\n",
    "\n",
    "        contexts.append(f\"[From: {source}]\\n{text}\\n\")\n",
    "        # Saves the chunk + source in a nice format for the final answer\n",
    "\n",
    "    return contexts\n",
    "    # Returns the list of text chunks so GPT can turn them into a natural answer\n",
    "\n",
    "# ================================\n",
    "# STEP 8: GENERATE ANSWER WITH OPENAI (v1.0+)\n",
    "# ================================\n",
    "def generate_answer(query: str, contexts: List[str]) -> str:\n",
    "    # Takes the user's question + retrieved chunks → returns a natural answer\n",
    "\n",
    "    print(\"\\nGenerating answer with OpenAI GPT...\")\n",
    "    # Tells you: \"Now asking GPT to write a nice response\"\n",
    "\n",
    "    if not contexts:\n",
    "        # No relevant info found in the manual\n",
    "        return \"I'm sorry, I couldn't find any information about that in the support manual.\"\n",
    "\n",
    "    context_str = \"\\n\\n\".join(contexts)\n",
    "    # Combines all retrieved chunks into one big string\n",
    "    # Each chunk already has \"[From: ...]\" and the text\n",
    "\n",
    "    system_prompt = f\"\"\"\n",
    "You are a helpful, accurate, and friendly support assistant for Supportco.\n",
    "Your knowledge comes ONLY from the Supportco Manual provided below.\n",
    "\n",
    "RULES:\n",
    "1. Answer the user's ORIGINAL QUESTION using ONLY the provided context.\n",
    "2. Use the original question to guide your tone, focus, and relevance.\n",
    "3. If the answer is not in the context, say: \"I don't have that information in the manual.\"\n",
    "4. Be concise, clear, and step-by-step.\n",
    "5. Cite the source (e.g., \"From: Supportco Manual.pdf\") when possible.\n",
    "6. Never guess or make up information.\n",
    "7. Always use normal and polite English - don't cite technical terms or abbreviations - always translate abbreviations into plain English\n",
    "8. Please don't ask users to perform additional steps unless you can describe the next steps from the provided context\n",
    "\n",
    "CONTEXT FROM SUPPORTCO MANUAL:\n",
    "{context_str.strip()}\n",
    "\"\"\"\n",
    "    # This is the AI's \"brain\" — its personality, rules, and knowledge\n",
    "    # It knows: \"Only answer from this text. Be nice. Be accurate.\"\n",
    "\n",
    "    try:\n",
    "        response = openai_client.chat.completions.create(\n",
    "            model=OPENAI_MODEL,                    # e.g., \"gpt-4o\" or \"gpt-3.5-turbo\"\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},  # ← AI's instructions + context\n",
    "                {\"role\": \"user\",   \"content\": query}          # ← The actual user question\n",
    "            ],\n",
    "            temperature=0.3,   # Low = consistent, accurate answers (not creative)\n",
    "            max_tokens=300     # Limit answer length\n",
    "        )\n",
    "        # Sends everything to OpenAI and gets back a natural response\n",
    "\n",
    "        answer = response.choices[0].message.content.strip()\n",
    "        # Extracts the actual text answer from OpenAI's response\n",
    "\n",
    "        return answer\n",
    "        # Returns the final, beautiful answer to show in the chat\n",
    "\n",
    "    except openai.AuthenticationError:\n",
    "        # Wrong or missing API key\n",
    "        return \"OpenAI API key is invalid. Check your .env file.\"\n",
    "\n",
    "    except openai.RateLimitError:\n",
    "        # Too many requests (free tier limit)\n",
    "        return \"OpenAI rate limit reached. Try again in a moment.\"\n",
    "\n",
    "    except Exception as e:\n",
    "        # Any other error (network, server, etc.)\n",
    "        return f\"Error with OpenAI: {str(e)}\"\n",
    "\n",
    "\n",
    "# ================================\n",
    "# GUI: Support Chat Interface\n",
    "# ================================\n",
    "class SupportChatGUI:\n",
    "    # This class builds and controls the entire chat window\n",
    "    \n",
    "    def __init__(self, root):\n",
    "        # __init__ runs when you do SupportChatGUI(root)\n",
    "        # root = the main Tkinter window (created with tk.Tk())\n",
    "        \n",
    "        self.root = root\n",
    "        # Save the main window so we can use it later\n",
    "\n",
    "        self.root.title(\"Supportco AI Support Assistant (OpenAI + RAG)\")\n",
    "        # Sets the title at the top of the window\n",
    "\n",
    "        self.root.geometry(\"850x650\")\n",
    "        # Sets window size: 850px wide, 650px tall\n",
    "\n",
    "        self.root.configure(bg=\"#f0f2f5\")\n",
    "        # Sets background color (light gray — clean look)\n",
    "\n",
    "        # === HEADER TEXT ===\n",
    "        header = tk.Label(\n",
    "            root,\n",
    "            text=\"Supportco AI Assistant\",\n",
    "            font=(\"Helvetica\", 16, \"bold\"),\n",
    "            bg=\"#f0f2f5\",\n",
    "            fg=\"#1a1a1a\"\n",
    "        )\n",
    "        header.pack(pady=10)\n",
    "        # Creates the big title at the top\n",
    "\n",
    "        # === CHAT DISPLAY BOX ===\n",
    "        self.chat_display = scrolledtext.ScrolledText(\n",
    "            root,\n",
    "            wrap=tk.WORD,           # Wrap text at word boundaries\n",
    "            width=90,\n",
    "            height=28,\n",
    "            font=(\"Helvetica\", 11),\n",
    "            bg=\"white\",\n",
    "            fg=\"#1a1a1a\",\n",
    "            state=tk.DISABLED       # User can't type directly in it\n",
    "        )\n",
    "        self.chat_display.pack(padx=20, pady=10, fill=tk.BOTH, expand=True)\n",
    "        # This is the main chat area where all messages appear\n",
    "\n",
    "        # === INPUT AREA (bottom) ===\n",
    "        input_frame = tk.Frame(root, bg=\"#f0f2f5\")\n",
    "        input_frame.pack(padx=20, pady=10, fill=tk.X)\n",
    "        # A container to hold the text box + Send button\n",
    "\n",
    "        self.entry = tk.Entry(\n",
    "            input_frame,\n",
    "            font=(\"Helvetica\", 12),\n",
    "            relief=tk.FLAT,\n",
    "            bg=\"white\",\n",
    "            fg=\"#1a1a1a\"\n",
    "        )\n",
    "        self.entry.pack(side=tk.LEFT, fill=tk.X, expand=True, padx=(0, 10))\n",
    "        # The box where the user types their question\n",
    "\n",
    "        self.entry.bind(\"<Return>\", self.send_message)\n",
    "        # Pressing Enter = same as clicking Send\n",
    "\n",
    "        send_btn = tk.Button(\n",
    "            input_frame,\n",
    "            text=\"Send\",\n",
    "            command=self.send_message,   # Click → run send_message()\n",
    "            bg=\"#007bff\",                # Blue background\n",
    "            fg=\"white\",\n",
    "            font=(\"Helvetica\", 11, \"bold\"),\n",
    "            relief=tk.FLAT,\n",
    "            cursor=\"hand2\"               # Hand cursor on hover\n",
    "        )\n",
    "        send_btn.pack(side=tk.RIGHT)\n",
    "        # The blue \"Send\" button\n",
    "\n",
    "        # === WELCOME MESSAGE ===\n",
    "        self.add_message(\"Assistant\", \"Hello! I'm your AI support assistant. Ask me anything about Supportco!\")\n",
    "        # Shows a friendly greeting when the app opens\n",
    "\n",
    "\n",
    "    def add_message(self, sender: str, message: str):\n",
    "        # Adds a new message to the chat display\n",
    "        self.chat_display.config(state=tk.NORMAL)     # Unlock to edit\n",
    "        self.chat_display.insert(tk.END, f\"{sender}: {message}\\n\\n\")\n",
    "        self.chat_display.config(state=tk.DISABLED)   # Lock again\n",
    "        self.chat_display.see(tk.END)                 # Auto-scroll to bottom\n",
    "\n",
    "\n",
    "    def send_message(self, event=None):\n",
    "        # Runs when user presses Enter or clicks Send\n",
    "        \n",
    "        query = self.entry.get().strip()\n",
    "        # Get what the user typed and remove extra spaces\n",
    "        \n",
    "        if not query:\n",
    "            return  # Do nothing if empty\n",
    "        \n",
    "        self.add_message(\"You\", query)\n",
    "        # Show the user's question in the chat\n",
    "        \n",
    "        self.entry.delete(0, tk.END)\n",
    "        # Clear the input box\n",
    "        \n",
    "        self.add_message(\"Assistant\", \"Searching manual and generating answer...\")\n",
    "        # Show \"thinking\" message\n",
    "\n",
    "        try:\n",
    "            contexts = search_qdrant(query)           # Step 1: Search your manual\n",
    "            answer = generate_answer(query, contexts) # Step 2: Ask GPT to write answer\n",
    "            # Use after() so GUI stays responsive\n",
    "            self.chat_display.after(100, lambda: self.add_message(\"Assistant\", answer))\n",
    "        except Exception as e:\n",
    "            # If anything goes wrong (no internet, bad key, etc.)\n",
    "            self.add_message(\"Assistant\", f\"Error: {e}\")\n",
    "\n",
    "# ================================\n",
    "# LAUNCH GUI\n",
    "# ================================\n",
    "if __name__ == \"__main__\":\n",
    "    # This line means: \"Only run the code below if this file is executed directly\"\n",
    "    # (e.g., you type: python support_chat.py)\n",
    "    # If someone imports this file, this part is skipped\n",
    "\n",
    "    if not Path(\"vector_db_built.flag\").exists():\n",
    "        # Check: Does the file \"vector_db_built.flag\" exist in the folder?\n",
    "        # This file is created by build_vector_db.py when it finishes successfully\n",
    "\n",
    "        messagebox.showerror(\n",
    "            \"Vector DB Not Found\",                                    # Title of pop-up\n",
    "            \"Please run build_vector_db.py first to create the vector database!\"\n",
    "        )\n",
    "        # If the flag is missing → show a red error pop-up and stop\n",
    "        # Prevents the app from starting without a knowledge base\n",
    "\n",
    "    else:\n",
    "        # The flag exists → everything is ready to go\n",
    "        \n",
    "        root = tk.Tk()\n",
    "        # Create the main window (the \"box\" that holds your entire app)\n",
    "        # root = tk.Tk() ← Step 1: Create the window (from Tkinter)\n",
    "\n",
    "        app = SupportChatGUI(root)\n",
    "        # Build your chat interface inside that window\n",
    "        # app = SupportChatGUI(root) ← Step 2: Put your chat inside the window\n",
    "        # This runs all the __init__ code: title, chat box, input field, etc.\n",
    "\n",
    "        root.mainloop()\n",
    "        # Starts the app\n",
    "        # root.mainloop() ← Step 3: Turn on the app\n",
    "        # mainloop() is inside the Tk class which is part of Tkinter which listens for mouse clicks, redraws the window etc.\n",
    "        # Keeps the window open and responsive until you close it\n",
    "        # This is the \"on\" switch for your entire AI assistant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680f7f98-e6b9-44d4-a4e5-62d880c0b422",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e62cba-2a87-4626-893b-a1f88aa92dea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (vector-search-venv)",
   "language": "python",
   "name": "vector-search-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
