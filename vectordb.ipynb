{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "804733de-88ad-4758-bdbf-4473000c38d0",
   "metadata": {},
   "source": [
    "\n",
    "```markdown\n",
    " AI Support-Chat Demo – Complete Setup Guide  \n",
    "\n",
    "> **Goal:** Ingest *Supportco Manual.pdf* → build a vector DB → launch a **live AI chat GUI** that answers questions using semantic search.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. System Prerequisites (run once per laptop)\n",
    "\n",
    "```bash\n",
    "# Homebrew (if you don’t have it)\n",
    "/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\n",
    "\n",
    "# Docker Desktop\n",
    "brew install --cask docker\n",
    "# → Open **Docker.app**, let it finish setup, keep it running\n",
    "\n",
    "# Python 3.11\n",
    "brew install python@3.11\n",
    "python3 --version   # should show 3.11.x\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Project folder\n",
    "\n",
    "```bash\n",
    "mkdir -p ~/VectorDB && cd ~/VectorDB\n",
    "```\n",
    "\n",
    "Place these files in the folder:\n",
    "\n",
    "* `build_vector_db.py`  \n",
    "* `support_chat.py`  \n",
    "* `support_docs/Supportco Manual.pdf` (or any PDFs/TXTs)\n",
    "* Any other documents loaded in the directory of PDF, TXT will automatically be loaded into the database\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Create & activate the virtual environment\n",
    "\n",
    "```bash\n",
    "python3 -m venv venv\n",
    "chmod +x venv/bin/activate      # fix permission if needed\n",
    "source venv/bin/activate\n",
    "```\n",
    "\n",
    "*Verify*  \n",
    "\n",
    "```bash\n",
    "which python   # → …/VectorDB/venv/bin/python\n",
    "pip --version\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Install **all** Python dependencies (once)\n",
    "\n",
    "```bash\n",
    "pip install --upgrade pip\n",
    "\n",
    "pip install \\\n",
    "  langchain \\\n",
    "  langchain-core \\\n",
    "  langchain-community \\\n",
    "  langchain-text-splitters \\\n",
    "  sentence-transformers \\\n",
    "  qdrant-client \\\n",
    "  openai \\\n",
    "  PyPDF2 \\\n",
    "  python-docx \\\n",
    "  \"unstructured[all-docs]\" \\\n",
    "  markdown \\\n",
    "  \"numpy<2\" \\\n",
    "  ipywidgets\n",
    "```\n",
    "\n",
    "*Quick sanity-check*\n",
    "\n",
    "```python\n",
    "from langchain_core.documents import Document\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from qdrant_client import QdrantClient\n",
    "print(\"All imports OK\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Start Qdrant (Docker)\n",
    "\n",
    "```bash\n",
    "mkdir -p qdrant_storage\n",
    "\n",
    "docker run -d \\\n",
    "  --name qdrant_demo \\\n",
    "  -p 6333:6333 -p 6334:6334 \\\n",
    "  -v $(pwd)/qdrant_storage:/qdrant/storage \\\n",
    "  qdrant/qdrant\n",
    "```\n",
    "\n",
    "*Verify*\n",
    "\n",
    "```bash\n",
    "curl http://localhost:6333\n",
    "# → JSON with \"title\":\"qdrant...\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Build the vector DB (run **once**)\n",
    "\n",
    "```bash\n",
    "python build_vector_db.py\n",
    "```\n",
    "\n",
    "Expected output:\n",
    "\n",
    "```\n",
    "Collection 'support_docs' created.\n",
    "  → Upserted batch 1\n",
    "Vector DB built successfully!\n",
    "SUCCESS! Run: python support_chat.py\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Launch the GUI chat\n",
    "\n",
    "```bash\n",
    "python support_chat.py\n",
    "```\n",
    "\n",
    "A window **“Supportco AI Support Assistant”** opens.  \n",
    "Try:  \n",
    "> `How do I reset my password?`  \n",
    "→ Answer appears instantly.\n",
    "\n",
    "---\n",
    "\n",
    "## 8. (Optional) Clean shutdown\n",
    "\n",
    "```bash\n",
    "docker stop qdrant_demo && docker rm qdrant_demo\n",
    "deactivate   # leave the venv\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Full Dependency List\n",
    "\n",
    "| Category | Package | Reason |\n",
    "|----------|---------|--------|\n",
    "| **Core** | `python@3.11` (brew) | Interpreter |\n",
    "| **System** | `docker` (brew cask) | Run Qdrant |\n",
    "| **LangChain** | `langchain`, `langchain-core`, `langchain-community`, `langchain-text-splitters` | RAG pipeline |\n",
    "| **Embeddings** | `sentence-transformers` | `all-MiniLM-L6-v2` |\n",
    "| **Vector DB** | `qdrant-client` | HTTP client |\n",
    "| **Docs** | `PyPDF2`, `python-docx`, `unstructured[all-docs]`, `markdown` | PDF/DOCX/MD/TXT |\n",
    "| **Numerics** | `numpy<2` | Compatibility |\n",
    "| **Widgets** | `ipywidgets` | Jupyter (optional) |\n",
    "| **GUI** | `tkinter` (built-in) | Desktop window |\n",
    "\n",
    "*One-liner (after `source venv/bin/activate`)*\n",
    "\n",
    "```bash\n",
    "pip install --upgrade pip && \\\n",
    "pip install langchain langchain-core langchain-community langchain-text-splitters \\\n",
    "            sentence-transformers qdrant-client openai \\\n",
    "            PyPDF2 python-docx \"unstructured[all-docs]\" markdown \\\n",
    "            \"numpy<2\" ipywidgets\n",
    "```\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0115e112-a5f9-46b7-bf76-1d39552485c3",
   "metadata": {},
   "source": [
    "\n",
    "## This version of build_vector_db.py and asupport_chat.py just have each section of the code clearly deliniated for a class demo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ec241b7-948c-4441-b4fc-000da654e82a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== BUILDING VECTOR DATABASE ===\n",
      "\n",
      "\n",
      "============================================================\n",
      "STEP 0: INGEST RAW DOCUMENTS\n",
      "============================================================\n",
      "Loading documents...\n",
      "  → SupportCo Online Support Personnel Instruction Manual.pdf\n",
      "Loaded 31 sections.\n",
      "\n",
      "============================================================\n",
      "STEP 1: CHUNKING\n",
      "============================================================\n",
      "Created 103 chunks.\n",
      "\n",
      "============================================================\n",
      "STEP 2: EMBEDDING\n",
      "============================================================\n",
      "Generating embeddings...\n",
      "  → Embedded 100/103 chunks\n",
      "Embedded 103 vectors.\n",
      "\n",
      "============================================================\n",
      "STEP 3: INDEXING & STORAGE IN QDRANT\n",
      "============================================================\n",
      "Deleted existing collection: support_docs\n",
      "Collection 'support_docs' created.\n",
      "  → Upserted batch 1\n",
      "  → Upserted batch 2\n",
      "Vector DB built successfully!\n",
      "\n",
      "============================================================\n",
      "SUCCESS! Vector DB is ready!\n",
      "Next: Run 'python support_chat.py' for the AI chat interface.\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================\n",
    "# build_vector_db.py - FINAL WORKING VERSION\n",
    "# PIPELINE: 0 → Ingest | 1 → Chunk | 2 → Embed | 3 → Index\n",
    "# ==============================================================\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "from langchain_community.document_loaders import PyPDFLoader, TextLoader\n",
    "from langchain_community.document_loaders import UnstructuredMarkdownLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http.models import (\n",
    "    Distance,\n",
    "    VectorParams,\n",
    "    HnswConfigDiff,\n",
    "    PointStruct,\n",
    ")\n",
    "\n",
    "# CONFIG\n",
    "DATA_DIR = \"support_docs\"\n",
    "COLLECTION_NAME = \"support_docs\"\n",
    "EMBEDDING_MODEL = \"all-MiniLM-L6-v2\"\n",
    "CHUNK_SIZE = 500\n",
    "CHUNK_OVERLAP = 50\n",
    "QDRANT_URL = \"http://localhost:6333\"\n",
    "\n",
    "\n",
    "# ==============================================================\n",
    "# STEP 0: INGEST RAW DOCUMENTS\n",
    "# Load PDFs, .txt, .md from support_docs/\n",
    "# ==============================================================\n",
    "def load_documents() -> List[Document]:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"STEP 0: INGEST RAW DOCUMENTS\")\n",
    "    print(\"=\"*60)\n",
    "    data_dir = Path(DATA_DIR)\n",
    "    if not data_dir.exists():\n",
    "        raise FileNotFoundError(f\"Create '{DATA_DIR}' folder with your files.\")\n",
    "    loaders = {\".pdf\": PyPDFLoader, \".txt\": TextLoader, \".md\": UnstructuredMarkdownLoader}\n",
    "    docs = []\n",
    "    print(\"Loading documents...\")\n",
    "    for file_path in data_dir.rglob(\"*\"):\n",
    "        ext = file_path.suffix.lower()\n",
    "        if ext in loaders:\n",
    "            print(f\"  → {file_path.name}\")\n",
    "            for doc in loaders[ext](str(file_path)).load():\n",
    "                doc.metadata.update({\"source\": file_path.name, \"category\": file_path.parent.name})\n",
    "                docs.append(doc)\n",
    "    print(f\"Loaded {len(docs)} sections.\")\n",
    "    return docs\n",
    "\n",
    "\n",
    "# ==============================================================\n",
    "# STEP 1: CHUNKING (content-aware)\n",
    "# Split long docs into 500-char chunks with 50 overlap\n",
    "# ==============================================================\n",
    "def chunk_documents(docs: List[Document]) -> List[Document]:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"STEP 1: CHUNKING\")\n",
    "    print(\"=\"*60)\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP)\n",
    "    chunks = splitter.split_documents(docs)\n",
    "    print(f\"Created {len(chunks)} chunks.\")\n",
    "    return chunks\n",
    "\n",
    "\n",
    "# ==============================================================\n",
    "# STEP 2: EMBEDDING\n",
    "# Convert each chunk into a 384-dim vector using all-MiniLM-L6-v2\n",
    "# ==============================================================\n",
    "def embed_chunks(chunks: List[Document]) -> List[dict]:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"STEP 2: EMBEDDING\")\n",
    "    print(\"=\"*60)\n",
    "    model = SentenceTransformer(EMBEDDING_MODEL)\n",
    "    embedded = []\n",
    "    print(\"Generating embeddings...\")\n",
    "    for i, c in enumerate(chunks):\n",
    "        vec = model.encode(c.page_content).tolist()\n",
    "        embedded.append({\n",
    "            \"id\": i,\n",
    "            \"vector\": vec,\n",
    "            \"payload\": {\n",
    "                \"text\": c.page_content,\n",
    "                \"source\": c.metadata.get(\"source\")\n",
    "            }\n",
    "        })\n",
    "        if i % 100 == 0 and i > 0:\n",
    "            print(f\"  → Embedded {i}/{len(chunks)} chunks\")\n",
    "    print(f\"Embedded {len(embedded)} vectors.\")\n",
    "    return embedded\n",
    "\n",
    "\n",
    "# ==============================================================\n",
    "# STEP 3: INDEXING & STORAGE IN QDRANT\n",
    "# Create collection + upsert all vectors\n",
    "# ==============================================================\n",
    "def build_qdrant_index(points: List[dict]):\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"STEP 3: INDEXING & STORAGE IN QDRANT\")\n",
    "    print(\"=\"*60)\n",
    "    client = QdrantClient(url=QDRANT_URL)\n",
    "    if client.collection_exists(COLLECTION_NAME):\n",
    "        client.delete_collection(COLLECTION_NAME)\n",
    "        print(f\"Deleted existing collection: {COLLECTION_NAME}\")\n",
    "    \n",
    "    client.create_collection(\n",
    "        collection_name=COLLECTION_NAME,\n",
    "        vectors_config=VectorParams(size=len(points[0][\"vector\"]), distance=Distance.COSINE),\n",
    "        hnsw_config=HnswConfigDiff(\n",
    "            m=16,\n",
    "            ef_construct=200,\n",
    "            full_scan_threshold=10000,\n",
    "        ),\n",
    "    )\n",
    "    print(f\"Collection '{COLLECTION_NAME}' created.\")\n",
    "\n",
    "    BATCH = 100\n",
    "    for i in range(0, len(points), BATCH):\n",
    "        batch = points[i:i+BATCH]\n",
    "        client.upsert(\n",
    "            collection_name=COLLECTION_NAME,\n",
    "            points=[PointStruct(id=p[\"id\"], vector=p[\"vector\"], payload=p[\"payload\"]) for p in batch]\n",
    "        )\n",
    "        print(f\"  → Upserted batch {i//BATCH + 1}\")\n",
    "    print(\"Vector DB built successfully!\")\n",
    "\n",
    "\n",
    "# ==============================================================\n",
    "# MAIN: Run all steps\n",
    "# ==============================================================\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"=== BUILDING VECTOR DATABASE ===\\n\")\n",
    "    docs = load_documents()\n",
    "    chunks = chunk_documents(docs)\n",
    "    points = embed_chunks(chunks)\n",
    "    build_qdrant_index(points)\n",
    "    Path(\"vector_db_built.flag\").touch()\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"SUCCESS! Vector DB is ready!\")\n",
    "    print(\"Next: Run 'python support_chat.py' for the AI chat interface.\")\n",
    "    print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ddbdf326-c743-464a-9ed8-fec2a6c5ea57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "INITIALIZING AI SUPPORT CHAT WITH OPENAI\n",
      "============================================================\n",
      "Loading embedding model...\n",
      "Searching Qdrant vector database...\n",
      "\n",
      " Embedding user query...\n",
      "  → Query vector generated (dim=384)\n",
      "  → Found 3 relevant chunks:\n",
      "     [1] Score: 0.413 | Source: SupportCo Online Support Personnel Instruction Manual.pdf\n",
      "     [2] Score: 0.400 | Source: SupportCo Online Support Personnel Instruction Manual.pdf\n",
      "     [3] Score: 0.352 | Source: SupportCo Online Support Personnel Instruction Manual.pdf\n",
      "\n",
      "Generating answer with OpenAI GPT...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/zr/8xlf44gd1yng__g6g_09_xnr0000gn/T/ipykernel_55703/3669287103.py:72: DeprecationWarning: `search` method is deprecated and will be removed in the future. Use `query_points` instead.\n",
      "  search_result = client.search(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching Qdrant vector database...\n",
      "\n",
      " Embedding user query...\n",
      "  → Query vector generated (dim=384)\n",
      "  → Found 3 relevant chunks:\n",
      "     [1] Score: 0.253 | Source: SupportCo Online Support Personnel Instruction Manual.pdf\n",
      "     [2] Score: 0.244 | Source: SupportCo Online Support Personnel Instruction Manual.pdf\n",
      "     [3] Score: 0.231 | Source: SupportCo Online Support Personnel Instruction Manual.pdf\n",
      "\n",
      "Generating answer with OpenAI GPT...\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================\n",
    "# support_chat.py - AI SUPPORT CHAT GUI with OpenAI GPT\n",
    "# Uses .env for API key | Modern OpenAI v1.0+ API\n",
    "# Answers questions using your PDF + GPT-3.5/GPT-4\n",
    "# Requires: vector DB built + Qdrant running\n",
    "# ==============================================================\n",
    "\n",
    "import tkinter as tk\n",
    "from tkinter import scrolledtext, messagebox\n",
    "from typing import List\n",
    "import os\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv  # ← Loads .env file\n",
    "\n",
    "# ================================\n",
    "# LOAD ENVIRONMENT VARIABLES\n",
    "# ================================\n",
    "load_dotenv()  # Reads .env file in project root\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not OPENAI_API_KEY:\n",
    "    raise ValueError(\n",
    "        \"OPENAI_API_KEY not found!\\n\"\n",
    "        \"Create a .env file in your project folder with:\\n\"\n",
    "        \"OPENAI_API_KEY=sk-...\\n\"\n",
    "        \"Or run: export OPENAI_API_KEY='sk-...'\"\n",
    "    )\n",
    "\n",
    "# ================================\n",
    "# CONFIG\n",
    "# ================================\n",
    "EMBEDDING_MODEL = \"all-MiniLM-L6-v2\"\n",
    "QDRANT_URL = \"http://localhost:6333\"\n",
    "COLLECTION_NAME = \"support_docs\"\n",
    "TOP_K = 3\n",
    "OPENAI_MODEL = \"gpt-5-chat-latest\"  # or gpt-3.5-turbo or \"gpt-4o\" for better quality\n",
    "\n",
    "# ================================\n",
    "# INITIALIZE CLIENTS\n",
    "# ================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"INITIALIZING AI SUPPORT CHAT WITH OPENAI\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from qdrant_client import QdrantClient\n",
    "from openai import OpenAI  # ← Modern OpenAI v1.0+\n",
    "\n",
    "print(\"Loading embedding model...\")\n",
    "model = SentenceTransformer(EMBEDDING_MODEL)\n",
    "client = QdrantClient(url=QDRANT_URL)\n",
    "openai_client = OpenAI(api_key=OPENAI_API_KEY)  # ← Secure client\n",
    "\n",
    "\n",
    "# ================================\n",
    "# STEP 2: EMBED USER QUERY\n",
    "# ================================\n",
    "def embed_query(query: str):\n",
    "    print(\"\\n Embedding user query...\")\n",
    "    query_vec = model.encode(query).tolist()\n",
    "    print(f\"  → Query vector generated (dim={len(query_vec)})\")\n",
    "    return query_vec\n",
    "\n",
    "\n",
    "# ================================\n",
    "# STEP 3: SEARCH QDRANT\n",
    "# ================================\n",
    "def search_qdrant(query: str) -> List[str]:\n",
    "    print(\"Searching Qdrant vector database...\")\n",
    "    query_vec = embed_query(query)\n",
    "    \n",
    "    search_result = client.search(\n",
    "        collection_name=COLLECTION_NAME,\n",
    "        query_vector=query_vec,\n",
    "        limit=TOP_K,\n",
    "        with_payload=True,\n",
    "    )\n",
    "    \n",
    "    contexts = []\n",
    "    print(f\"  → Found {len(search_result)} relevant chunks:\")\n",
    "    for i, hit in enumerate(search_result):\n",
    "        text = hit.payload.get(\"text\", \"\")\n",
    "        source = hit.payload.get(\"source\", \"Unknown\")\n",
    "        score = hit.score\n",
    "        print(f\"     [{i+1}] Score: {score:.3f} | Source: {source}\")\n",
    "        contexts.append(f\"[From: {source}]\\n{text}\\n\")\n",
    "    \n",
    "    return contexts\n",
    "\n",
    "\n",
    "# ================================\n",
    "# STEP 4: GENERATE ANSWER WITH OPENAI (v1.0+)\n",
    "# ================================\n",
    "def generate_answer(query: str, contexts: List[str]) -> str:\n",
    "    print(\"\\nGenerating answer with OpenAI GPT...\")\n",
    "\n",
    "    if not contexts:\n",
    "        return \"I'm sorry, I couldn't find any information about that in the support manual.\"\n",
    "\n",
    "    context_str = \"\\n\\n\".join(contexts)\n",
    "\n",
    "    system_prompt = f\"\"\"\n",
    "You are a helpful, accurate, and friendly support assistant for Supportco.\n",
    "Your knowledge comes ONLY from the Supportco Manual provided below.\n",
    "\n",
    "RULES:\n",
    "1. Answer the user's ORIGINAL QUESTION using ONLY the provided context.\n",
    "2. Use the original question to guide your tone, focus, and relevance.\n",
    "3. If the answer is not in the context, say: \"I don't have that information in the manual.\"\n",
    "4. Be concise, clear, and step-by-step.\n",
    "5. Cite the source (e.g., \"From: Supportco Manual.pdf\") when possible.\n",
    "6. Never guess or make up information.\n",
    "7. Always use normal and polite english - don't cite technical terms or abbreviations - always translate abbreviations into plain english\n",
    "8. please don't ask users to perform additional steps unless you can describe the next steps from the provided context\n",
    "\n",
    "CONTEXT FROM SUPPORTCO MANUAL:\n",
    "{context_str.strip()}\n",
    "\"\"\"\n",
    "\n",
    "    try:\n",
    "        response = openai_client.chat.completions.create(\n",
    "            model=OPENAI_MODEL,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": query}  # ← USER QUESTION\n",
    "            ],\n",
    "            temperature=0.3,\n",
    "            max_tokens=300\n",
    "        )\n",
    "        answer = response.choices[0].message.content.strip()\n",
    "        return answer\n",
    "\n",
    "    except openai.AuthenticationError:\n",
    "        return \"OpenAI API key is invalid. Check your .env file.\"\n",
    "    except openai.RateLimitError:\n",
    "        return \"OpenAI rate limit reached. Try again in a moment.\"\n",
    "    except Exception as e:\n",
    "        return f\"Error with OpenAI: {str(e)}\"\n",
    "\n",
    "\n",
    "# ================================\n",
    "# GUI: Support Chat Interface\n",
    "# ================================\n",
    "class SupportChatGUI:\n",
    "    def __init__(self, root):\n",
    "        self.root = root\n",
    "        self.root.title(\"Supportco AI Support Assistant (OpenAI + RAG)\")\n",
    "        self.root.geometry(\"850x650\")\n",
    "        self.root.configure(bg=\"#f0f2f5\")\n",
    "\n",
    "        # Header\n",
    "        header = tk.Label(\n",
    "            root, \n",
    "            text=\"Supportco AI Assistant\", \n",
    "            font=(\"Helvetica\", 16, \"bold\"),\n",
    "            bg=\"#f0f2f5\",\n",
    "            fg=\"#1a1a1a\"\n",
    "        )\n",
    "        header.pack(pady=10)\n",
    "\n",
    "        # Chat display\n",
    "        self.chat_display = scrolledtext.ScrolledText(\n",
    "            root,\n",
    "            wrap=tk.WORD,\n",
    "            width=90,\n",
    "            height=28,\n",
    "            font=(\"Helvetica\", 11),\n",
    "            bg=\"white\",\n",
    "            fg=\"#1a1a1a\",\n",
    "            state=tk.DISABLED\n",
    "        )\n",
    "        self.chat_display.pack(padx=20, pady=10, fill=tk.BOTH, expand=True)\n",
    "\n",
    "        # Input frame\n",
    "        input_frame = tk.Frame(root, bg=\"#f0f2f5\")\n",
    "        input_frame.pack(padx=20, pady=10, fill=tk.X)\n",
    "\n",
    "        self.entry = tk.Entry(\n",
    "            input_frame,\n",
    "            font=(\"Helvetica\", 12),\n",
    "            relief=tk.FLAT,\n",
    "            bg=\"white\",\n",
    "            fg=\"#1a1a1a\"\n",
    "        )\n",
    "        self.entry.pack(side=tk.LEFT, fill=tk.X, expand=True, padx=(0, 10))\n",
    "        self.entry.bind(\"<Return>\", self.send_message)\n",
    "\n",
    "        send_btn = tk.Button(\n",
    "            input_frame,\n",
    "            text=\"Send\",\n",
    "            command=self.send_message,\n",
    "            bg=\"#007bff\",\n",
    "            fg=\"white\",\n",
    "            font=(\"Helvetica\", 11, \"bold\"),\n",
    "            relief=tk.FLAT,\n",
    "            cursor=\"hand2\"\n",
    "        )\n",
    "        send_btn.pack(side=tk.RIGHT)\n",
    "\n",
    "        # Welcome\n",
    "        self.add_message(\"Assistant\", \"Hello! I'm your AI support assistant. Ask me anything about Supportco!\")\n",
    "\n",
    "    def add_message(self, sender: str, message: str):\n",
    "        self.chat_display.config(state=tk.NORMAL)\n",
    "        self.chat_display.insert(tk.END, f\"{sender}: {message}\\n\\n\")\n",
    "        self.chat_display.config(state=tk.DISABLED)\n",
    "        self.chat_display.see(tk.END)\n",
    "\n",
    "    def send_message(self, event=None):\n",
    "        query = self.entry.get().strip()\n",
    "        if not query:\n",
    "            return\n",
    "        \n",
    "        self.add_message(\"You\", query)\n",
    "        self.entry.delete(0, tk.END)\n",
    "        self.add_message(\"Assistant\", \"Searching manual and generating answer...\")\n",
    "\n",
    "        try:\n",
    "            contexts = search_qdrant(query)\n",
    "            answer = generate_answer(query, contexts)\n",
    "            self.chat_display.after(100, lambda: self.add_message(\"Assistant\", answer))\n",
    "        except Exception as e:\n",
    "            self.add_message(\"Assistant\", f\"Error: {e}\")\n",
    "\n",
    "\n",
    "# ================================\n",
    "# LAUNCH GUI\n",
    "# ================================\n",
    "if __name__ == \"__main__\":\n",
    "    if not Path(\"vector_db_built.flag\").exists():\n",
    "        messagebox.showerror(\n",
    "            \"Vector DB Not Found\",\n",
    "            \"Please run build_vector_db.py first to create the vector database!\"\n",
    "        )\n",
    "    else:\n",
    "        root = tk.Tk()\n",
    "        app = SupportChatGUI(root)\n",
    "        root.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680f7f98-e6b9-44d4-a4e5-62d880c0b422",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e62cba-2a87-4626-893b-a1f88aa92dea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (vector-search-venv)",
   "language": "python",
   "name": "vector-search-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
